{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSPZpAgsVBFz",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tensor basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkai7yQzRkbT",
    "outputId": "1d2a2c46-f18b-4b88-c194-06e8123c8315",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4757, 0.2972, 0.3359, 0.7571, 0.4397, 0.9612, 0.8614, 0.7999, 0.9163,\n",
      "         0.2609],\n",
      "        [0.0319, 0.1693, 0.2241, 0.7564, 0.1713, 0.0724, 0.2746, 0.8384, 0.8252,\n",
      "         0.0385],\n",
      "        [0.1166, 0.6669, 0.7649, 0.5202, 0.3242, 0.2848, 0.8230, 0.2095, 0.9417,\n",
      "         0.3675],\n",
      "        [0.1946, 0.9519, 0.5732, 0.3648, 0.9523, 0.5763, 0.4093, 0.4550, 0.3520,\n",
      "         0.9308],\n",
      "        [0.5053, 0.7228, 0.0222, 0.6617, 0.6374, 0.9093, 0.8455, 0.0105, 0.9149,\n",
      "         0.6252],\n",
      "        [0.9029, 0.4969, 0.3513, 0.9745, 0.6178, 0.0509, 0.5198, 0.8683, 0.8409,\n",
      "         0.1675],\n",
      "        [0.9060, 0.0812, 0.9914, 0.2675, 0.1016, 0.0801, 0.8217, 0.5446, 0.6334,\n",
      "         0.1474],\n",
      "        [0.3398, 0.4277, 0.8579, 0.1888, 0.9959, 0.8948, 0.1136, 0.0185, 0.0420,\n",
      "         0.9009],\n",
      "        [0.7401, 0.7818, 0.4084, 0.6793, 0.4212, 0.4924, 0.5928, 0.3100, 0.7951,\n",
      "         0.8554]])\n",
      "tensor([[1.4757, 1.2972, 1.3359, 1.7571, 1.4397, 1.9612, 1.8614, 1.7999, 1.9163,\n",
      "         1.2609],\n",
      "        [1.0319, 1.1693, 1.2241, 1.7564, 1.1713, 1.0724, 1.2746, 1.8384, 1.8252,\n",
      "         1.0385],\n",
      "        [1.1166, 1.6669, 1.7649, 1.5202, 1.3242, 1.2848, 1.8230, 1.2095, 1.9417,\n",
      "         1.3675],\n",
      "        [1.1946, 1.9519, 1.5732, 1.3648, 1.9523, 1.5763, 1.4093, 1.4550, 1.3520,\n",
      "         1.9308],\n",
      "        [1.5053, 1.7228, 1.0222, 1.6617, 1.6374, 1.9093, 1.8455, 1.0105, 1.9149,\n",
      "         1.6252],\n",
      "        [1.9029, 1.4969, 1.3513, 1.9745, 1.6178, 1.0509, 1.5198, 1.8683, 1.8409,\n",
      "         1.1675],\n",
      "        [1.9060, 1.0812, 1.9914, 1.2675, 1.1016, 1.0801, 1.8217, 1.5446, 1.6334,\n",
      "         1.1474],\n",
      "        [1.3398, 1.4277, 1.8579, 1.1888, 1.9959, 1.8948, 1.1136, 1.0185, 1.0420,\n",
      "         1.9009],\n",
      "        [1.7401, 1.7818, 1.4084, 1.6793, 1.4212, 1.4924, 1.5928, 1.3100, 1.7951,\n",
      "         1.8554]])\n",
      "1.1945748329162598\n",
      "tensor([1.4757, 1.2972, 1.3359, 1.7571, 1.4397, 1.9612, 1.8614, 1.7999, 1.9163,\n",
      "        1.2609])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as npy\n",
    "x = torch.empty(9,10)\n",
    "x1 = torch.rand(9,10)\n",
    "x2 = torch.zeros(9,10)\n",
    "x3 = torch.ones(9,10,dtype=torch.int)\n",
    "x4 = torch.tensor([1.23,4.4])\n",
    "c1 = x1.numpy()\n",
    "c2 = torch.from_numpy(c1)\n",
    "print(c2)\n",
    "'''\n",
    "print(x)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x3.size())\n",
    "'''\n",
    "y = x1+x3\n",
    "y1 = x1.add(x3)\n",
    "y2 = torch.add(x1,x3)\n",
    "x1.add_(x3) # In Pytorch .method_ all perform inplace operations\n",
    "#print(y)\n",
    "print(y1)\n",
    "print(y1[3][0].item()) # .item() returns that particular value that is not in tensor from\n",
    "print(y1[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk27jjVmVI5G",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l0GrpxrnU_bH",
    "outputId": "e35e8cfd-2453-453a-a140-13cbefbf8039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7482, -0.6281, -0.1982],\n",
      "        [ 0.8449,  0.9261,  1.7667],\n",
      "        [-0.7757,  0.4253,  0.4431]], requires_grad=True)\n",
      "tensor([[4.7482, 1.3719, 1.8018],\n",
      "        [2.8449, 2.9261, 3.7667],\n",
      "        [1.2243, 2.4253, 2.4431]], grad_fn=<AddBackward0>)\n",
      "tensor([[45.0908,  3.7641,  6.4931],\n",
      "        [16.1872, 17.1237, 28.3766],\n",
      "        [ 2.9977, 11.7639, 11.9377]], grad_fn=<MulBackward0>)\n",
      "None\n",
      "tensor([[1.8993e+00, 5.4875e+00, 7.2073e-01],\n",
      "        [1.2518e+01, 1.1704e-01, 1.5067e+01],\n",
      "        [4.8971e-03, 9.7011e-01, 9.7725e+00]])\n",
      "None\n",
      "tensor([3., 3., 3.])\n",
      "None\n",
      "tensor([3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "#z = z.mean()\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "v = torch.tensor([[0.1,1.0,0.1],[1.1,0.01,1.0],[0.001,0.1,1.0]])\n",
    "print(z.backward(v))\n",
    "'''\n",
    ".backward() method without argument only gives one value so can only be used when tensor is scalar\n",
    " otherwise pass a vector through backward method which does a jacobian product and gives all gradient values\n",
    "'''\n",
    "# To stop calculating grad function recursively we can use 3 methods\n",
    "# 1) x.requires_grad_(False) --> Inplace operation\n",
    "# 2) y = x.detach() --> New new tensor to store the value\n",
    "'''\n",
    " 3) wrap all statements in a with statement that is\n",
    "    with torch.no_grad():\n",
    "      y = x+2\n",
    "      print(x)\n",
    " '''\n",
    "print(x.grad)\n",
    "weights = torch.ones(3,requires_grad=True)\n",
    "vect = torch.tensor([0.1,0.1,0.1])\n",
    "for epoch in range(2):\n",
    "  model_output = (weights*3).sum()\n",
    "  print(model_output.backward())\n",
    "  #print(model_output.backward(vect))\n",
    "  # Here .backward() accumulates grads for every iteration hence need to use above 3 methods to remove them\n",
    "  print(weights.grad)\n",
    "  weights.grad.zero_() # Makes grad values zeroes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riajr7aWJVHA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysgj7lwmJbm0",
    "outputId": "a037fac3-fdfa-4fe4-80ad-eba2715864f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 4.], grad_fn=<PowBackward0>)\n",
      "None\n",
      "tensor([-4.,  8.])\n",
      "tensor([0., 0.])\n",
      "tensor([ 5., -6.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1.0,2.0])\n",
    "y = torch.tensor([3.0,2.0])\n",
    "w = torch.tensor([1.0,2.0],requires_grad=True)\n",
    "vect = torch.ones(2)\n",
    "y_hat = w*x\n",
    "loss = (y_hat-y)**2\n",
    "print(loss)\n",
    "print(loss.backward(vect))\n",
    "#print(loss.sum().item())\n",
    "with torch.no_grad():\n",
    "    w -= w.grad\n",
    "#w = w - w.grad\n",
    "print(w.grad)\n",
    "w.grad.zero_()\n",
    "print(w.grad)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzrAhvIlqD97",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TetA1y6E33EK"
   },
   "source": [
    "## Using Numpy (Manually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfptCdSQqMWx",
    "outputId": "4bc45611-6c6d-4c75-ca6e-ccaf4865a2ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4.]\n",
      "[2. 4. 6. 8.]\n",
      "Prediction before training: f(5) = 0.000\n",
      "epoch = 1: w = 0.300, loss = 30.00000000\n",
      "epoch = 11: w = 1.665, loss = 1.16278565\n",
      "epoch = 21: w = 1.934, loss = 0.04506905\n",
      "epoch = 31: w = 1.987, loss = 0.00174685\n",
      "epoch = 41: w = 1.997, loss = 0.00006770\n",
      "epoch = 51: w = 1.999, loss = 0.00000262\n",
      "epoch = 61: w = 2.000, loss = 0.00000010\n",
      "epoch = 71: w = 2.000, loss = 0.00000000\n",
      "epoch = 81: w = 2.000, loss = 0.00000000\n",
      "epoch = 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# f = w*x --> it is the function\n",
    "# f = 2*x --> need to train the model and predict w as 2 from X and Y data\n",
    "X = np.array([1,2,3,4],dtype=np.float32)\n",
    "Y = np.array([2,4,6,8],dtype=np.float32)\n",
    "print(X)\n",
    "print(Y)\n",
    "w = 0.0\n",
    "def forward(x):\n",
    "  return w*x\n",
    "def loss(y,y_predicted):\n",
    "  return ((y_predicted-y)**2).mean()\n",
    "def gradient(x,y,y_predicted):\n",
    "  return np.dot(2*x,y_predicted-y)/len(X)\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "for epoch in range(n_iters):\n",
    "  y_pred = forward(X)\n",
    "  l = loss(Y,y_pred)\n",
    "  dw = gradient(X,Y,y_pred)\n",
    "  w -= learning_rate*dw\n",
    "  if epoch % 10 == 0:\n",
    "    print(f'epoch = {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mif1r7Cw38da"
   },
   "source": [
    "## Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIDi381v_OK8"
   },
   "source": [
    "### With manual forward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATF8TwBE4GYM",
    "outputId": "eef42a0d-3545-4b06-dd3c-e13a7ebca10e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4.])\n",
      "tensor([2., 4., 6., 8.])\n",
      "Prediction before training: f(5) = 0.000\n",
      "epoch = 1: w = 0.300, b = 0.100, loss = 30.00000000\n",
      "epoch = 301: w = 1.916, b = 0.247, loss = 0.01027267\n",
      "epoch = 601: w = 1.966, b = 0.101, loss = 0.00169971\n",
      "epoch = 901: w = 1.986, b = 0.041, loss = 0.00028123\n",
      "epoch = 1201: w = 1.994, b = 0.017, loss = 0.00004653\n",
      "epoch = 1501: w = 1.998, b = 0.007, loss = 0.00000770\n",
      "epoch = 1801: w = 1.999, b = 0.003, loss = 0.00000127\n",
      "epoch = 2101: w = 2.000, b = 0.001, loss = 0.00000021\n",
      "epoch = 2401: w = 2.000, b = 0.000, loss = 0.00000004\n",
      "epoch = 2701: w = 2.000, b = 0.000, loss = 0.00000001\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# f = w*x\n",
    "# f = 2*x\n",
    "X = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "print(X)\n",
    "print(Y)\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "b = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "def forward(x):\n",
    "  return w*x+b\n",
    "'''\n",
    "def loss(y,y_predicted):\n",
    "  return ((y_predicted-y)**2).mean()\n",
    "def gradient(x,y,y_predicted):\n",
    "  return np.dot(2*x,y_predicted-y).mean()\n",
    "'''\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "learning_rate = 0.01\n",
    "n_iters = 3000\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w,b],lr=learning_rate) # SGD--> Stochaistic Gradient descent\n",
    "for epoch in range(n_iters):\n",
    "  y_pred = forward(X)\n",
    "  l = loss(Y,y_pred)\n",
    "  l.backward()\n",
    "  '''\n",
    "  with torch.no_grad():\n",
    "      w -= learning_rate*w.grad\n",
    "  '''\n",
    "  optimizer.step()\n",
    "  #w.grad.zero_()\n",
    "  optimizer.zero_grad()\n",
    "  if epoch % 300 == 0:\n",
    "    print(f'epoch = {epoch+1}: w = {w:.3f}, b = {b:.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIJ4aksJ_W8f"
   },
   "source": [
    "### With Automatic Forward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gRdOItiu_dRA",
    "outputId": "30752fae-51ad-4ed7-facd-1a1242007b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.]])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.],\n",
      "        [8.]])\n",
      "4 1\n",
      "Prediction before training: f(5) = 5.537\n",
      "epoch = 1: w = 1.114, loss = 5.04093647\n",
      "epoch = 11: w = 1.630, loss = 0.23526834\n",
      "epoch = 21: w = 1.720, loss = 0.10483180\n",
      "epoch = 31: w = 1.741, loss = 0.09570965\n",
      "epoch = 41: w = 1.751, loss = 0.09006073\n",
      "epoch = 51: w = 1.758, loss = 0.08481669\n",
      "epoch = 61: w = 1.765, loss = 0.07987986\n",
      "epoch = 71: w = 1.772, loss = 0.07523041\n",
      "epoch = 81: w = 1.779, loss = 0.07085162\n",
      "epoch = 91: w = 1.786, loss = 0.06672770\n",
      "Prediction after training: f(5) = 9.570\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# f = w*x\n",
    "# f = 2*x\n",
    "# X = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "# Y = torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "# Need to have row number as number of samples and coulmn number as number of features\n",
    "X = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "print(X)\n",
    "print(Y)\n",
    "X_test = torch.tensor([5],dtype=torch.float32)\n",
    "n_samples,n_features = X.shape\n",
    "print(n_samples,n_features)\n",
    "in_size = n_features\n",
    "out_size = n_features\n",
    "#model = nn.Linear(in_size,out_size)\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "  def __init__(self,input_dim,output_dim):\n",
    "    super(LinearRegression,self).__init__()\n",
    "    # define layers\n",
    "    self.lin =  nn.Linear(input_dim,output_dim)\n",
    "  def forward(self,x):\n",
    "    return self.lin(x)\n",
    "\n",
    "model = LinearRegression(in_size,out_size)\n",
    "'''\n",
    "w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True)\n",
    "def forward(x):\n",
    "  return w*x\n",
    "def loss(y,y_predicted):\n",
    "  return ((y_predicted-y)**2).mean()\n",
    "def gradient(x,y,y_predicted):\n",
    "  return np.dot(2*x,y_predicted-y).mean()\n",
    "'''\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD([w],lr=learning_rate) # SGD--> Stochaistic Gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "for epoch in range(n_iters):\n",
    "  #y_pred = forward(X)\n",
    "  y_pred = model(X)\n",
    "  l = loss(Y,y_pred)\n",
    "  l.backward()\n",
    "  '''\n",
    "  with torch.no_grad():\n",
    "      w -= learning_rate*w.grad\n",
    "  '''\n",
    "  optimizer.step()\n",
    "  #w.grad.zero_()\n",
    "  optimizer.zero_grad()\n",
    "  if epoch % 10 == 0:\n",
    "    [w,b] = model.parameters()\n",
    "    #print(f'epoch = {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "    print(f'epoch = {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3P5KcgzRBmm",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tztzfoHiRE8N",
    "outputId": "6045442e-cbee-423d-cd78-7890f119309c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model before training: [[-0.60114044]\n",
      " [-0.51947904]\n",
      " [-0.3532005 ]\n",
      " [-0.29157054]\n",
      " [-0.1208038 ]\n",
      " [-0.37820333]\n",
      " [-0.34769946]\n",
      " [-0.61454606]\n",
      " [-0.77797663]\n",
      " [-0.39215863]\n",
      " [-0.6068279 ]\n",
      " [-0.55269086]\n",
      " [-0.7113026 ]\n",
      " [-0.26028275]\n",
      " [-0.71113855]\n",
      " [-0.2767061 ]\n",
      " [-0.48031458]\n",
      " [-0.54266226]\n",
      " [-0.6144145 ]\n",
      " [-0.7050717 ]\n",
      " [-0.5322861 ]\n",
      " [ 0.0292207 ]\n",
      " [-0.2682708 ]\n",
      " [-0.41859937]\n",
      " [-0.5498292 ]\n",
      " [-0.61735874]\n",
      " [-0.45183575]\n",
      " [-0.33195123]\n",
      " [-0.18095902]\n",
      " [-0.6332838 ]\n",
      " [-0.54200715]\n",
      " [-0.6633066 ]\n",
      " [-0.42124453]\n",
      " [-0.26584584]\n",
      " [-0.27422535]\n",
      " [-0.5769632 ]\n",
      " [-0.54077023]\n",
      " [-0.08190098]\n",
      " [-0.9819222 ]\n",
      " [-0.25346696]\n",
      " [ 0.00999433]\n",
      " [-0.13380817]\n",
      " [-0.6537467 ]\n",
      " [-0.6611048 ]\n",
      " [-0.53593963]\n",
      " [-0.263824  ]\n",
      " [-0.42722163]\n",
      " [-0.20827943]\n",
      " [-0.54785305]\n",
      " [-0.35005105]\n",
      " [-0.510358  ]\n",
      " [-0.4362079 ]\n",
      " [-0.33109313]\n",
      " [-0.39139152]\n",
      " [-0.61914635]\n",
      " [-0.30598783]\n",
      " [-0.19311637]\n",
      " [-0.50852454]\n",
      " [-0.3437727 ]\n",
      " [-0.29606998]\n",
      " [-0.37088242]\n",
      " [-0.4364671 ]\n",
      " [-0.63481814]\n",
      " [-0.36785316]\n",
      " [-0.39564347]\n",
      " [-0.72074425]\n",
      " [-0.42156112]\n",
      " [-0.6741547 ]\n",
      " [-0.60299826]\n",
      " [-0.7150638 ]\n",
      " [-0.7887069 ]\n",
      " [-0.51338524]\n",
      " [-0.09724894]\n",
      " [-0.32420212]\n",
      " [-0.61813504]\n",
      " [-0.44585687]\n",
      " [-0.46613887]\n",
      " [-0.60721886]\n",
      " [-0.20779717]\n",
      " [-0.07010263]\n",
      " [-0.26011717]\n",
      " [-0.9275247 ]\n",
      " [-0.41046086]\n",
      " [-0.5021406 ]\n",
      " [-0.4358333 ]\n",
      " [-0.2053287 ]\n",
      " [-0.4909775 ]\n",
      " [-0.63165236]\n",
      " [-0.39892966]\n",
      " [-0.9189753 ]\n",
      " [-0.41143456]\n",
      " [-0.52365184]\n",
      " [-0.58230513]\n",
      " [-0.20876288]\n",
      " [-0.42026338]\n",
      " [-0.53045833]\n",
      " [-0.089259  ]\n",
      " [-0.4534607 ]\n",
      " [-0.45377237]\n",
      " [-0.5065139 ]]\n",
      "epoch: 100, loss = 562.0214\n",
      "epoch: 200, loss = 342.4986\n",
      "epoch: 300, loss = 333.0059\n",
      "epoch: 400, loss = 332.5870\n",
      "epoch: 500, loss = 332.5684\n",
      "epoch: 600, loss = 332.5676\n",
      "epoch: 700, loss = 332.5675\n",
      "epoch: 800, loss = 332.5676\n",
      "epoch: 900, loss = 332.5676\n",
      "epoch: 1000, loss = 332.5676\n",
      "Model after training: [[ -46.40622  ]\n",
      " [ -16.515078 ]\n",
      " [  44.349113 ]\n",
      " [  66.90798  ]\n",
      " [ 129.41502  ]\n",
      " [  35.197124 ]\n",
      " [  46.36269  ]\n",
      " [ -51.313187 ]\n",
      " [-111.13491  ]\n",
      " [  30.088968 ]\n",
      " [ -48.488064 ]\n",
      " [ -28.671867 ]\n",
      " [ -86.729706 ]\n",
      " [  78.36049  ]\n",
      " [ -86.66967  ]\n",
      " [  72.34893  ]\n",
      " [  -2.1794224]\n",
      " [ -25.001015 ]\n",
      " [ -51.26503  ]\n",
      " [ -84.44897  ]\n",
      " [ -21.202957 ]\n",
      " [ 184.32964  ]\n",
      " [  75.43656  ]\n",
      " [  20.41066  ]\n",
      " [ -27.624386 ]\n",
      " [ -52.34273  ]\n",
      " [   8.244895 ]\n",
      " [  52.127136 ]\n",
      " [ 107.39596  ]\n",
      " [ -58.1719   ]\n",
      " [ -24.761223 ]\n",
      " [ -69.16136  ]\n",
      " [  19.442432 ]\n",
      " [  76.324196 ]\n",
      " [  73.25698  ]\n",
      " [ -37.556442 ]\n",
      " [ -24.308462 ]\n",
      " [ 143.65492  ]\n",
      " [-185.78665  ]\n",
      " [  80.85532  ]\n",
      " [ 177.29207  ]\n",
      " [ 124.654945 ]\n",
      " [ -65.66211  ]\n",
      " [ -68.355446 ]\n",
      " [ -22.54029  ]\n",
      " [  77.06427  ]\n",
      " [  17.254595 ]\n",
      " [  97.39567  ]\n",
      " [ -26.901047 ]\n",
      " [  45.501926 ]\n",
      " [ -13.176432 ]\n",
      " [  13.965277 ]\n",
      " [  52.441235 ]\n",
      " [  30.369759 ]\n",
      " [ -52.997074 ]\n",
      " [  61.63071  ]\n",
      " [ 102.94591  ]\n",
      " [ -12.505327 ]\n",
      " [  47.800034 ]\n",
      " [  65.26102  ]\n",
      " [  37.87686  ]\n",
      " [  13.870396 ]\n",
      " [ -58.733536 ]\n",
      " [  38.985676 ]\n",
      " [  28.813383 ]\n",
      " [ -90.185715 ]\n",
      " [  19.326546 ]\n",
      " [ -73.132195 ]\n",
      " [ -47.086258 ]\n",
      " [ -88.10647  ]\n",
      " [-115.06261  ]\n",
      " [ -14.284517 ]\n",
      " [ 138.03699  ]\n",
      " [  54.963596 ]\n",
      " [ -52.626877 ]\n",
      " [  10.4333935]\n",
      " [   3.0094247]\n",
      " [ -48.63115  ]\n",
      " [  97.57218  ]\n",
      " [ 147.97357  ]\n",
      " [  78.4211   ]\n",
      " [-165.8751   ]\n",
      " [  23.38966  ]\n",
      " [ -10.168558 ]\n",
      " [  14.102392 ]\n",
      " [  98.47574  ]\n",
      " [  -6.082455 ]\n",
      " [ -57.574745 ]\n",
      " [  27.610512 ]\n",
      " [-162.74571  ]\n",
      " [  23.033249 ]\n",
      " [ -18.042496 ]\n",
      " [ -39.511803 ]\n",
      " [  97.218704 ]\n",
      " [  19.801577 ]\n",
      " [ -20.533913 ]\n",
      " [ 140.96162  ]\n",
      " [   7.6501093]\n",
      " [   7.536024 ]\n",
      " [ -11.76936  ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Design Model (input size, output size, forward pass)\n",
    "# 2) Contruct optimizer and loss\n",
    "# 3) Training Loop\n",
    "#  - forward loop: Compute prediction and loss\n",
    "#  - backward loop: gradients\n",
    "#  - update weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "X_numpy,Y_numpy = datasets.make_regression(n_samples=100,n_features=1,noise=20,random_state=1)\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "Y = torch.from_numpy(Y_numpy.astype(np.float32))\n",
    "Y = Y.view(Y.shape[0],1)\n",
    "n_samples,n_features = X.shape\n",
    "'''\n",
    "print(X_numpy)\n",
    "print(Y_numpy)\n",
    "print(X)\n",
    "print(Y)\n",
    "'''\n",
    "# Linear model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size,output_size)\n",
    "print(f\"Model before training: {model(X).detach().numpy()}\")\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "# Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  # forward pass and loss\n",
    "  y_pred = model(X)\n",
    "  loss = criterion(y_pred,Y)\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  # update values\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  if(epoch+1)%100==0:\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "trained_model = model(X).detach().numpy()\n",
    "print(f\"Model after training: {trained_model}\")\n",
    "plt.plot(X_numpy,Y_numpy,'ro')\n",
    "plt.plot(X_numpy,trained_model,'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrxSjCrNgbOi",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Logistical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vkf9smA0ghPP",
    "outputId": "823ce01b-b27a-457e-b460-1a13d2b3f182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 1, loss = 0.5499\n",
      "epoch: 101, loss = 0.2403\n",
      "epoch: 201, loss = 0.1783\n",
      "epoch: 301, loss = 0.1490\n",
      "epoch: 401, loss = 0.1314\n",
      "epoch: 501, loss = 0.1195\n",
      "epoch: 601, loss = 0.1107\n",
      "epoch: 701, loss = 0.1040\n",
      "epoch: 801, loss = 0.0986\n",
      "epoch: 901, loss = 0.0941\n",
      "accuracy: 93.8596\n"
     ]
    }
   ],
   "source": [
    "# 1) Design Model (input size, output size, forward pass)\n",
    "# 2) Contruct optimizer and loss\n",
    "# 3) Training Loop\n",
    "#  - forward loop: Compute prediction and loss\n",
    "#  - backward loop: gradients\n",
    "#  - update weights\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 0) preparing the data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X,Y = bc.data,bc.target\n",
    "n_samples,n_features = X.shape\n",
    "print(n_samples,n_features)\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2,random_state=1234)\n",
    "# Scaling the data\n",
    "sc = StandardScaler()\n",
    "X_test = sc.fit_transform(X_test)\n",
    "X_train = sc.fit_transform(X_train)\n",
    "# array to tensor format conversion\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "Y_test = torch.from_numpy(Y_test.astype(np.float32))\n",
    "Y_train = torch.from_numpy(Y_train.astype(np.float32))\n",
    "# Rearranging Y values\n",
    "Y_test = Y_test.view(Y_test.shape[0],1)\n",
    "Y_train = Y_train.view(Y_train.shape[0],1)\n",
    "# 1) model\n",
    "# f = w*x+b and sigmoid function at the end\n",
    "class LogisticalRegression(nn.Module):\n",
    "  def __init__(self,n_input_features):\n",
    "    super(LogisticalRegression,self).__init__()\n",
    "    # define layers\n",
    "    self.linear = nn.Linear(n_input_features,1)\n",
    "  def forward(self,x):\n",
    "    y_predicted = torch.sigmoid(self.linear(x))\n",
    "    return y_predicted\n",
    "model = LogisticalRegression(n_features)\n",
    "\n",
    "# 2) Loss and Optimizer, loss function is binary crossentropy\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "\n",
    "# 3) Training Loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "  # forwars pass and loss\n",
    "  Y_predicted = model(X_train)\n",
    "  loss = criterion(Y_predicted,Y_train)\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  # update of values\n",
    "  optimizer.step()\n",
    "  # zero grad\n",
    "  optimizer.zero_grad()\n",
    "  if (epoch%100==0):\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "# Time to test the accuracy of the model\n",
    "with torch.no_grad():\n",
    "  Y_predicted = model(X_test)\n",
    "  Y_pred_cls = Y_predicted.round()\n",
    "  acc = ((Y_pred_cls.eq(Y_test).sum())/float(Y_test.shape[0]))*100\n",
    "  print(f'accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atKpC4SItpUn",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WriTjTPbtuCn"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "epoch = 1 forward and backward pass of ALL training samples\n",
    "batch_size = number of training samples in one forward and backward pass\n",
    "number of iterations = number of passes, each pass using [batch_size] number of samples\n",
    "eg: 100 sanples, batch_size 20 => 100/20 = 5 iterations for 1 epoch\n",
    "'''\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import math\n",
    "import pandas\n",
    "class TrojanData(Dataset):\n",
    "  def __init__(self):\n",
    "    xy = np.loadtxt('Trojan_Detection.csv',delimiter=',',skiprows=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYpCVEhrShH8",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obzBp4w4Sjq-"
   },
   "outputs": [],
   "source": [
    "# Binary classification\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Multiclass problem\n",
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at the end\n",
    "        return out\n",
    "\n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss()  # (applies Softmax)\n",
    "'''\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZA8Q8axahOH",
    "tags": []
   },
   "source": [
    "# Trojan Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ1qar2zal7z",
    "outputId": "4d63d616-bff1-43a3-b841-4a0d199efa5b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in X_train: 141985\n",
      "Number of records in X_test: 35497\n",
      "Number of records in Y_train: 141985\n",
      "Number of records in Y_test: 35497\n",
      "Total number of samples: 177482\n",
      "Number of features: 85\n",
      "Before Training: tensor([[0.5824],\n",
      "        [0.5219],\n",
      "        [0.5079],\n",
      "        ...,\n",
      "        [0.5248],\n",
      "        [0.5631],\n",
      "        [0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "epoch: 1, loss = 0.6951\n",
      "epoch: 301, loss = 0.0985\n",
      "epoch: 601, loss = 0.0555\n",
      "epoch: 901, loss = 0.0378\n",
      "epoch: 1201, loss = 0.0279\n",
      "epoch: 1501, loss = 0.0219\n",
      "epoch: 1801, loss = 0.0180\n",
      "epoch: 2101, loss = 0.0151\n",
      "epoch: 2401, loss = 0.0129\n",
      "epoch: 2701, loss = 0.0112\n",
      "epoch: 3001, loss = 0.0098\n",
      "After training: tensor([[9.9996e-01],\n",
      "        [1.0822e-06],\n",
      "        [1.0000e+00],\n",
      "        ...,\n",
      "        [9.9993e-01],\n",
      "        [1.5981e-05],\n",
      "        [9.9999e-01]])\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "accuracy: 99.9915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "from sklearn import preprocessing\n",
    "df = pd.read_csv(\"Trojan Detection/Trojan_Detection.csv\", sep = r',', skipinitialspace = True)\n",
    "df = df.dropna()\n",
    "df.drop([\"Unnamed: 0\"], axis = 1).values\n",
    "df = df.replace(\"Trojan\", 1)\n",
    "df = df.replace(\"Benign\", 0)\n",
    "number = preprocessing.LabelEncoder()\n",
    "df[\"Flow ID\"] = number.fit_transform(df[\"Flow ID\"])\n",
    "df[\"Source IP\"] = number.fit_transform(df[\"Source IP\"])\n",
    "df[\"Destination IP\"] = number.fit_transform(df[\"Destination IP\"])\n",
    "df[\"Timestamp\"] = number.fit_transform(df[\"Timestamp\"])\n",
    "# Columns used as predictors\n",
    "X = df.drop([\"Class\"], axis = 1).values\n",
    "Y = df[\"Class\"].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0, test_size = 0.2)\n",
    "sc = StandardScaler()\n",
    "X_test = sc.fit_transform(X_test)\n",
    "X_train = sc.fit_transform(X_train)\n",
    "# Transforming dataframes to tensors\n",
    "X_test = torch.tensor(X_test,dtype=(torch.float))\n",
    "X_train = torch.tensor(X_train,dtype=(torch.float))\n",
    "Y_test = torch.tensor(Y_test,dtype=(torch.float))\n",
    "Y_train = torch.tensor(Y_train,dtype=(torch.float))\n",
    "Y_test = Y_test.view(Y_test.shape[0],1)\n",
    "Y_train = Y_train.view(Y_train.shape[0],1)\n",
    "n_samples,n_features = X.shape\n",
    "X_train_number = X_train.shape[0]\n",
    "X_test_number = X_test.shape[0]\n",
    "Y_train_number = Y_train.shape[0]\n",
    "Y_test_number = Y_test.shape[0]\n",
    "print(f'Number of records in X_train: {X_train_number}')\n",
    "print(f'Number of records in X_test: {X_test_number}')\n",
    "print(f'Number of records in Y_train: {Y_train_number}')\n",
    "print(f'Number of records in Y_test: {Y_test_number}')\n",
    "print(f'Total number of samples: {n_samples}') # samples-> 177482, features-> 85\n",
    "print(f'Number of features: {n_features}')\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size, bias = True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at the end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "\n",
    "model = Model(input_size=n_features, hidden_size=5)\n",
    "\n",
    "# 2) Loss and Optimizer, loss function is binary crossentropy\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)\n",
    "Y_before_training = model(X_test)\n",
    "print(f'Before Training: {Y_before_training}')\n",
    "# 3) Training Loop\n",
    "num_epochs = 3300\n",
    "for epoch in range(num_epochs):\n",
    "  # forwars pass and loss\n",
    "  Y_predicted = model(X_train)\n",
    "  loss = criterion(Y_predicted,Y_train)\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  # update of values\n",
    "  optimizer.step()\n",
    "  # zero grad\n",
    "  optimizer.zero_grad()\n",
    "  if (epoch%300==0):\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "# Time to test the accuracy of the model\n",
    "with torch.no_grad():\n",
    "  Y_predicted = model(X_test)\n",
    "  Y_pred_cls = Y_predicted.round()\n",
    "  acc = ((Y_pred_cls.eq(Y_test).sum())/float(Y_test.shape[0]))*100\n",
    "  print(f'After training: {Y_predicted}') # predicted values\n",
    "  print(Y_test) # Actual class value in Trojan dataset\n",
    "  print(f'accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1i4Z60nx8XA",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Malware detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cRRTpu_HyB1X",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual class labels of Training data: tensor([2, 8, 9,  ..., 4, 4, 4])\n",
      "Labels starting from 0 of Training data: tensor([1, 7, 8,  ..., 3, 3, 3])\n",
      "X_train size: torch.Size([10868, 1804])\n",
      "Y_train size: torch.Size([10868, 1])\n",
      "9\n",
      "Number of records in X_train: 10868\n",
      "Number of records in X_test: 10873\n",
      "Number of records in Y_train: 10868\n",
      "Total number of samples: 10868\n",
      "Number of features: 1804\n",
      "Before Training prediction of Training data: tensor([[0.1227, 0.1100, 0.1139,  ..., 0.0939, 0.1056, 0.1204],\n",
      "        [0.1127, 0.1187, 0.1101,  ..., 0.1125, 0.1135, 0.1128],\n",
      "        [0.1148, 0.1090, 0.1116,  ..., 0.1207, 0.1072, 0.1022],\n",
      "        ...,\n",
      "        [0.1135, 0.1184, 0.1038,  ..., 0.1064, 0.1060, 0.1182],\n",
      "        [0.1095, 0.1126, 0.1108,  ..., 0.1070, 0.1157, 0.1109],\n",
      "        [0.1189, 0.1160, 0.1103,  ..., 0.1091, 0.1062, 0.1186]])\n",
      "Before training prediction of Training data: tensor([0, 1, 6,  ..., 1, 7, 0])\n",
      "epoch: 1, loss = 2.1949\n",
      "epoch: 51, loss = 0.1705\n",
      "epoch: 101, loss = 0.0819\n",
      "epoch: 151, loss = 0.0519\n",
      "epoch: 201, loss = 0.0372\n",
      "epoch: 251, loss = 0.0284\n",
      "epoch: 301, loss = 0.0227\n",
      "epoch: 351, loss = 0.0187\n",
      "epoch: 401, loss = 0.0157\n",
      "epoch: 451, loss = 0.0134\n",
      "After Training prediction of Training data: tensor([[3.6305e-06, 9.9997e-01, 3.0574e-06,  ..., 5.5493e-06, 1.2264e-06,\n",
      "         6.7767e-06],\n",
      "        [5.1417e-05, 1.3127e-06, 7.8270e-08,  ..., 2.9146e-06, 9.9971e-01,\n",
      "         1.2467e-04],\n",
      "        [1.4677e-05, 2.6088e-05, 5.5482e-06,  ..., 2.8858e-04, 9.7566e-06,\n",
      "         9.9955e-01],\n",
      "        ...,\n",
      "        [1.4426e-04, 7.7236e-04, 3.1778e-04,  ..., 9.6450e-04, 5.9944e-03,\n",
      "         1.4852e-03],\n",
      "        [2.5736e-04, 4.6153e-04, 5.0224e-05,  ..., 4.2958e-04, 6.6524e-03,\n",
      "         4.7900e-04],\n",
      "        [2.2790e-04, 8.6449e-04, 2.9233e-04,  ..., 1.2035e-03, 4.4224e-03,\n",
      "         2.0965e-03]])\n",
      "After training prediction of Training data: tensor([2, 8, 9,  ..., 4, 4, 4])\n",
      "accuracy: 99.7792\n",
      "After Training prediction of Test data: tensor([[2.6748e-04, 5.8861e-04, 9.2256e-04,  ..., 1.5755e-03, 2.1311e-02,\n",
      "         5.4787e-04],\n",
      "        [6.5616e-04, 1.2317e-03, 7.1406e-04,  ..., 1.7296e-03, 4.8622e-02,\n",
      "         2.8700e-03],\n",
      "        [4.1603e-05, 3.3013e-04, 1.6654e-04,  ..., 1.2166e-03, 6.3590e-03,\n",
      "         4.3748e-04],\n",
      "        ...,\n",
      "        [6.4388e-05, 3.1973e-05, 9.9958e-01,  ..., 5.6899e-05, 2.3169e-05,\n",
      "         5.3569e-05],\n",
      "        [3.2158e-05, 9.9946e-01, 1.4146e-04,  ..., 3.7815e-05, 3.2465e-06,\n",
      "         3.0390e-04],\n",
      "        [6.3147e-05, 6.8182e-07, 4.8829e-08,  ..., 1.8318e-06, 9.9974e-01,\n",
      "         1.1519e-04]])\n",
      "After training prediction of Test data: tensor([5, 5, 5,  ..., 3, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn as nn\n",
    "df_train = pd.read_csv(\"Malware Dataset/Dataset/Dataset/train/LargeTrain.csv\", sep = r',')\n",
    "df_train = df_train.dropna()\n",
    "df_test = pd.read_csv(\"Malware Dataset/Dataset/Dataset/test/LargeTest.csv\", sep = r',')\n",
    "df_test = df_test.dropna()\n",
    "# Columns used as predictors\n",
    "X_train = df_train.drop([\"Class\"], axis = 1).values\n",
    "Y_train = df_train[\"Class\"].values\n",
    "X_test = df_test\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "# Transforming dataframes to tensors\n",
    "X_test = torch.tensor(X_test,dtype=(torch.float))\n",
    "X_train = torch.tensor(X_train,dtype=(torch.float))\n",
    "#Y_test = torch.tensor(Y_test,dtype=(torch.float))\n",
    "Y_train = torch.tensor(Y_train,dtype=(torch.long))\n",
    "print(f'Actual class labels of Training data: {Y_train}')\n",
    "Y_train = Y_train-1\n",
    "print(f'Labels starting from 0 of Training data: {Y_train}')\n",
    "#print(Y_train)\n",
    "#Y_test = Y_test.view(Y_test.shape[0],1)\n",
    "Y_train = Y_train.view(Y_train.shape[0],1)\n",
    "#Y_train = Y_train.squeeze().view(-1, 1)\n",
    "print(f'X_train size: {X_train.shape}')\n",
    "print(f'Y_train size: {Y_train.shape}')\n",
    "n_samples,n_features = X_train.shape\n",
    "X_train_number = X_train.shape[0]\n",
    "X_test_number = X_test.shape[0]\n",
    "Y_train_number = Y_train.shape[0]\n",
    "hidden_size = 1000\n",
    "num_classes = len(Y_train.unique()) # num_classes = 9\n",
    "print(num_classes)\n",
    "#Y_test_number = Y_test.shape[0]\n",
    "print(f'Number of records in X_train: {X_train_number}')\n",
    "print(f'Number of records in X_test: {X_test_number}')\n",
    "print(f'Number of records in Y_train: {Y_train_number}')\n",
    "#print(f'Number of records in Y_test: {Y_test_number}')\n",
    "print(f'Total number of samples: {n_samples}') # samples-> 10868, features->1084 \n",
    "print(f'Number of features: {n_features}')\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.l1 = nn.Linear(input_size, hidden_size, bias = True)\n",
    "        self.l2 = nn.Linear(hidden_size, 600, bias = True)\n",
    "        self.leak_relu = nn.LeakyReLU(0.02)\n",
    "        self.l3 = nn.Linear(600, num_classes, bias = True)\n",
    "        '''\n",
    "        self.l4 = nn.Linear(800, 700, bias = True)\n",
    "        self.l5 = nn.Linear(700, 600, bias = True)\n",
    "        self.l6 = nn.Linear(600, 500, bias = True)\n",
    "        self.l7 = nn.Linear(500, 400, bias = True)\n",
    "        self.l8 = nn.Linear(400, 300, bias = True)\n",
    "        self.l9 = nn.Linear(300, 200, bias = True)\n",
    "        self.l10 = nn.Linear(200, 100, bias = True)\n",
    "        self.l11 = nn.Linear(100, num_classes, bias = True)\n",
    "        '''\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l3(out)\n",
    "        '''\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l6(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l7(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l8(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l9(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l10(out)\n",
    "        out = self.leak_relu(out)\n",
    "        out = self.l11(out)\n",
    "        '''\n",
    "        #out = nn.Softmax(dim=1)(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(n_features, hidden_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 500\n",
    "with torch.no_grad():\n",
    " Y_predicted_logits = model(X_train)\n",
    " Y_predicted_check = torch.softmax(Y_predicted_logits,dim = 1)\n",
    " print(f'Before Training prediction of Training data: {Y_predicted_check}')\n",
    " Y_predicted = torch.argmax(Y_predicted_check, dim=1)\n",
    " print(f'Before training prediction of Training data: {Y_predicted}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # forwars pass and loss\n",
    "  Y_logits = model(X_train)\n",
    "  #Y_pred = torch.softmax(Y_logits,dim=1).argmax(dim = 1)\n",
    "  #loss = criterion(Y_logits, torch.max(Y_train, 1)[0])\n",
    "  loss = criterion(Y_logits, Y_train.squeeze())\n",
    "  # backward pass\n",
    "  loss.backward()\n",
    "  # update of values\n",
    "  optimizer.step()\n",
    "  # zero grad\n",
    "  optimizer.zero_grad()\n",
    "  if (epoch%50==0):\n",
    "    print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n",
    "with torch.no_grad():\n",
    " Y_trained_predicted = model(X_train)\n",
    " Y_trained_predicted_check = torch.softmax(Y_trained_predicted,dim = 1)\n",
    " print(f'After Training prediction of Training data: {Y_trained_predicted_check}')\n",
    " Y_trained_prediction = torch.argmax(Y_trained_predicted_check, dim=1)\n",
    " print(f'After training prediction of Training data: {Y_trained_prediction+1}')\n",
    " sum=0 \n",
    " for i in range(len(Y_trained_prediction)):\n",
    "  if Y_trained_prediction[i] == Y_train[i]:\n",
    "   sum = sum + 1\n",
    " acc = (sum/len(Y_train))*100\n",
    " print(f'accuracy: {acc:.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    " X_trained_predicted = model(X_test)\n",
    " X_trained_predicted_check = torch.softmax(X_trained_predicted,dim = 1)\n",
    " print(f'After Training prediction of Test data: {X_trained_predicted_check}')\n",
    " X_trained_prediction = torch.argmax(X_trained_predicted_check, dim=1)\n",
    " print(f'After training prediction of Test data: {X_trained_prediction+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 256. KiB for an array with shape (32768,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNewYork Taxi Dataset/yellow_tripdata_2019-01.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNewYork Taxi Dataset/yellow_tripdata_2019-02.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mdf3 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-03.csv\", sep = r',')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mdf4 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-04.csv\", sep = r',')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03mdf18 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-06.csv\", sep = r',')\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1036\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1090\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1192\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mA:\\Anaconda\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1801\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 256. KiB for an array with shape (32768,) and data type int64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-01.csv\", sep = r',')\n",
    "df2 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-02.csv\", sep = r',')\n",
    "'''\n",
    "df3 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-03.csv\", sep = r',')\n",
    "df4 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-04.csv\", sep = r',')\n",
    "df5 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-05.csv\", sep = r',')\n",
    "df6 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-06.csv\", sep = r',')\n",
    "df7 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-07.csv\", sep = r',')\n",
    "df8 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-08.csv\", sep = r',')\n",
    "df9 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-09.csv\", sep = r',')\n",
    "df10 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-10.csv\", sep = r',')\n",
    "df11 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-11.csv\", sep = r',')\n",
    "df12 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2019-12.csv\", sep = r',')\n",
    "df13 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-01.csv\", sep = r',')\n",
    "df14 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-02.csv\", sep = r',')\n",
    "df15 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-03.csv\", sep = r',')\n",
    "df16 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-04.csv\", sep = r',')\n",
    "df17 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-05.csv\", sep = r',')\n",
    "df18 = pd.read_csv(\"NewYork Taxi Dataset/yellow_tripdata_2020-06.csv\", sep = r',')\n",
    "'''\n",
    "X_test = torch.tensor(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tSPZpAgsVBFz",
    "Hk27jjVmVI5G",
    "riajr7aWJVHA",
    "xzrAhvIlqD97",
    "TetA1y6E33EK",
    "fIDi381v_OK8",
    "XIJ4aksJ_W8f",
    "S3P5KcgzRBmm",
    "SrxSjCrNgbOi",
    "atKpC4SItpUn",
    "fYpCVEhrShH8",
    "vZA8Q8axahOH"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
